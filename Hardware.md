# ðŸ› ï¸ Hardware Guide (Windows & macOS)

This guide helps you pick **practical hardware** for training/fineâ€‘tuning and running LLMs locally using LM Studio, Ollama, and Hugging Face tooling. It focuses on **consumerâ€‘grade setups** that balance speed, cost, and stability.

---

## TL;DR Recommendations

- **Beginner (inference + tiny fineâ€‘tunes)**  
  - **Windows:** RTX 4060 (8â€“16GB VRAM), 32GB RAM, 1TB NVMe  
  - **macOS:** MacBook Air/Pro **M2/M3 (16â€“24GB unified)** for inference; **M3 Pro/Max (36â€“64GB)** for small QLoRA

- **Serious (7Bâ€“8B QLoRA, comfy inference)**  
  - **Windows:** **RTX 4070 Ti SUPER 16GB** or **RTX 4080 SUPER 16GB**, 64GB RAM, 2TB+ NVMe  
  - **macOS:** **M3 Max 48â€“64GB** unified for smooth 7B/8B and 4â€‘bit QLoRA

- **Power (13B+ QLoRA, faster epochs, multiâ€‘model work)**  
  - **Windows:** **RTX 4090 24GB**, 128GB RAM, 4TB+ NVMe (Gen4)  
  - **macOS:** **M3 Max 64â€“128GB** unified; still slower than a 4090 for training, great for dev & inference

> If youâ€™re unsure: a **Windows tower + NVIDIA 16â€“24GB VRAM** gives the best overall compatibility and speed for training today.

---

## How Model Size Maps to Hardware

| Model (params) | Typical Quant (inference) | VRAM (Windows, GPU) | Unified Mem (macOS) | Notes |
|---|---:|---:|---:|---|
| 3â€“4B | Q4_K_M | 4â€“6GB | 8â€“12GB | Snappy on any modern GPU / Mâ€‘series |
| 7B | Q4_K_M / Q5_K_M | 8â€“12GB | 16â€“24GB | â€œSweet spotâ€ for local use |
| 8B | Q4_K_M / Q5_K_M | 10â€“14GB | 24â€“36GB | Llamaâ€‘3.1â€‘8B strong generalist |
| 13B | Q4_K_M | 16â€“20GB | 36â€“48GB | Heavier context â†’ more memory |
| 70B | Q4 (split / CPU offload) | 48GB+ (multiâ€‘GPU ideal) | 96â€“192GB | Not beginner friendly locally |

> **Training (QLoRA)** needs **less VRAM** than full fineâ€‘tuning, but you still want **â‰¥16GB VRAM** for 7â€“8B models + 4k ctx.

---

## Windows: Detailed Recommendations

### GPU (the main bottleneck)
- **Great value:** RTX **4070 Ti SUPER 16GB** â†’ best price/perf for 7â€“8B QLoRA + fast inference  
- **High end:** RTX **4080 SUPER 16GB** or **4090 24GB** â†’ faster tokens/sec, bigger batches, 13B more comfortable  
- **Budget:** RTX **4060 16GB** (ok), **4060 8GB** (compromises on ctx/batch)  
- **Avoid:** AMD on Windows for training (ROCm Windows support is immature)

### CPU
- **Modern 12â€“24 thread CPU** (Intel i7â€‘13700/14700 or AMD Ryzen 7 7700/7800X3D).  
  Helps with data preprocessing, tokenization, and multiâ€‘worker dataloaders.

### RAM
- **Minimum 32GB**, **64GB recommended**, **128GB** if you juggle large datasets or run multiple VMs/containers.

### Storage
- **NVMe Gen4 SSD** (7GB/s class)  
  - **1TB** minimal; **2TBâ€“4TB** recommended for models + checkpoints + datasets.  
  - Consider **2â€‘drive setup**: OS/Apps (1TB) + Data/Models (2â€“4TB).  
  - SSD Endurance: Prefer **â‰¥1200 TBW** if you train often.

### Power & Thermals
- **PSU:** 850W (4080) or 1000â€“1200W (4090) 80+ Gold/Platinum  
- **Cooling:** 240â€“360mm AIO or highâ€‘end air; ensure **case airflow** (2 intake, 1â€“2 exhaust)

### Example Builds

**Balanced 7â€“8B Trainer**


### Windows + WSL2 Notes
- Install **WSL2 (Ubuntu 22.04)** for best Python/CLI ecosystem compatibility.  
- NVIDIA drivers on Windows automatically expose the GPU to WSL2.  
- Use **Conda envs** per project; pin **CUDAâ€‘enabled PyTorch**.  
- For llama.cpp builds: use **CMake + AVX2**; for GPU, compile with **CUDA**.

---

## macOS: Detailed Recommendations

### Chips & Memory
- **M2/M3 Base (8â€“24GB)** â†’ great **inference** with Ollama/LM Studio (Metal), light QLoRA on tiny models  
- **M3 Pro (36â€“48GB)** â†’ viable for 7B inference with larger ctx, small QLoRA runs  
- **M3 Max (48â€“128GB)** â†’ best mac choice; 7â€“8B comfortable, 13B possible with compromises

> macOS uses **Unified Memory**, shared by CPU/GPU/Neural Engine. More unified memory = larger models / context windows.

### Tools (macâ€‘native)
- **Ollama** and **LM Studio** use **Metal** (no CUDA required).  
- **llama.cpp** compiled with **Metal** backend for best performance.  
- **Apple MLX** (Python) is emerging for Appleâ€‘optimized training/inference.

### eGPU?
- **Not supported** on Apple Silicon. Donâ€™t plan on external GPUs for Mâ€‘series Macs.
- **Supported** on Windows machines with OCulink or Thunderbolt 3/4 (USB-C).

### Example macOS Picks

**Portable Dev / Inference**

